{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD $R = P \\cdot Q^T$ via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we want to obtain the following factorization\n",
    "\\begin{equation}\n",
    "    \\widehat{r}_{x,i} = q_i \\cdot p_x = \\sum_{f \\in F} q_{if} \\cdot p_{fx}\n",
    "\\end{equation}\n",
    "where $f \\in F$ are hidden factors.\n",
    "Our goal is to find the matrix $P$ and $Q$ such that\n",
    "\\begin{equation}\n",
    "  \\min_{P,Q} \\sum_{(i, x) \\in R} (\\widehat{r}_{x,i} - q_i \\cdot p_x)^2 + \\left[ \\lambda_1 \\sum_{x} \\|p_x\\|^2 + \\lambda_2 \\sum_{i} \\|q_i\\|^2 \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent: Initialize the matrix $P$ and $Q$ (set the missing values in $R$ to 0).\n",
    "\n",
    "Repeat:\n",
    "\n",
    "  * $P = P - \\alpha \\cdot \\nabla P$,\n",
    "      * where $\\nabla P$ is the gradient / derivative of the matrix $P$: $\\nabla P = [\\nabla p_{j, x}]$ and $\\nabla p_{j,x} = \\sum_{i, x}\\big( -2  (r_{x, i} - q_i \\cdot p_x) q_{i, j} + 2 \\lambda_1 p_{j, x}\\big)$, and\n",
    "  * $Q = Q - \\alpha \\cdot \\nabla Q$,\n",
    "      * where $\\nabla Q$ is the gradient / derivative of the matrix $Q$: $\\nabla Q = [\\nabla q_{i, j}]$ and $\\nabla q_{i, j} = \\sum_{i, x}\\big( -2  (r_{x, i} - q_i \\cdot p_x) p_{j, x} + 2 \\lambda_2 q_{i, j}\\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent (GD) vs. Stochastic GD:\n",
    "\n",
    "  * $\\nabla q_{i,j} = \\sum_{x, i} \\nabla Q(r_{x,i})$.\n",
    "  * $Q = Q - \\alpha \\cdot \\nabla Q = Q - \\alpha \\cdot \\left( \\sum_{x, i} \\nabla Q(r_{x,i}) \\right)$,\n",
    "  * GD: $Q = Q - \\alpha \\cdot \\left( \\sum_{x, i} \\nabla Q(r_{x,i}) \\right)$,\n",
    "  * SGD: $Q = Q - \\alpha \\cdot \\nabla Q(r_{x,i})$, Instead of calculating the gradient for all directions, we do each step separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
